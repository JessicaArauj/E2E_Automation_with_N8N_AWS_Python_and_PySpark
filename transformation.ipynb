{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JessicaArauj/E2E_Automation_with_N8N_AWS_Python_and_PySpark/blob/main/transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "XiQm_3X2Ded8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.0 boto3\n",
        "!pip install boto3"
      ],
      "metadata": {
        "id": "mxVrzJG2DVzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "tL-8ZUdmDi0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "465e1731-fa78-4ff1-951d-212957f777e9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jrzOsiLkKewy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "import boto3\n",
        "import glob\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array, struct, explode, col, lit, split, udf\n",
        "from pyspark.sql.types import IntegerType, DoubleType, StringType, LongType\n",
        "\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variables"
      ],
      "metadata": {
        "id": "QBJvN5DpDlLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AWS_ACCESS_KEY_ID = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "AWS_REGION = userdata.get('AWS_REGION')\n",
        "BUCKET_NAME = userdata.get('BUCKET_NAME')"
      ],
      "metadata": {
        "id": "XTEmrm30Dyqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing, transformation, and loading pipeline using PySpark and AWS S3"
      ],
      "metadata": {
        "id": "v4g1TtkGEC1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = boto3.client(\n",
        "    \"s3\",\n",
        "    region_name=AWS_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        ")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"S3DataTransformation\").getOrCreate()\n",
        "\n",
        "months_map = {\n",
        "    \"1\": \"Janeiro\",\n",
        "    \"2\": \"Fevereiro\",\n",
        "    \"3\": \"Março\",\n",
        "    \"4\": \"Abril\",\n",
        "    \"5\": \"Maio\",\n",
        "    \"6\": \"Junho\",\n",
        "    \"7\": \"Julho\",\n",
        "    \"8\": \"Agosto\",\n",
        "    \"9\": \"Setembro\",\n",
        "    \"10\": \"Outubro\",\n",
        "    \"11\": \"Novembro\",\n",
        "    \"12\": \"Dezembro\",\n",
        "}\n",
        "month_udf = udf(lambda m: months_map.get(str(int(m)), m) if m else None, StringType())\n",
        "\n",
        "uf_to_region = {\n",
        "    \"AC\": \"Norte\", \"AP\": \"Norte\", \"AM\": \"Norte\", \"PA\": \"Norte\",\n",
        "    \"RO\": \"Norte\", \"RR\": \"Norte\", \"TO\": \"Norte\",\n",
        "    \"MA\": \"Nordeste\", \"PI\": \"Nordeste\", \"CE\": \"Nordeste\", \"RN\": \"Nordeste\",\n",
        "    \"PB\": \"Nordeste\", \"PE\": \"Nordeste\", \"AL\": \"Nordeste\", \"SE\": \"Nordeste\",\n",
        "    \"BA\": \"Nordeste\",\n",
        "    \"MG\": \"Sudeste\", \"SP\": \"Sudeste\", \"RJ\": \"Sudeste\", \"ES\": \"Sudeste\",\n",
        "    \"PR\": \"Sul\", \"RS\": \"Sul\", \"SC\": \"Sul\",\n",
        "    \"MT\": \"Centro-Oeste\", \"MS\": \"Centro-Oeste\", \"GO\": \"Centro-Oeste\", \"DF\": \"Centro-Oeste\",\n",
        "}\n",
        "\n",
        "\n",
        "def uf_to_region_func(uf):\n",
        "    if uf == \"Brasil\":\n",
        "        return \"Brasil\"\n",
        "    return uf_to_region.get(uf, \"Unknown\")\n",
        "\n",
        "\n",
        "uf_to_region_udf = udf(uf_to_region_func, StringType())\n",
        "\n",
        "\n",
        "def process_normal_csv(df):\n",
        "    if \"Ano\" in df.columns:\n",
        "        df = df.withColumn(\"Ano\", col(\"Ano\").cast(StringType()))\n",
        "    if \"Mês\" in df.columns:\n",
        "        df = df.withColumn(\"Mês\", month_udf(col(\"Mês\")))\n",
        "\n",
        "    string_cols = [\n",
        "        \"Grupo Econômico\", \"Empresa\", \"CNPJ\", \"Porte da Prestadora\",\n",
        "        \"UF\", \"Município\", \"Código IBGE Município\", \"Faixa de Velocidade\",\n",
        "        \"Tecnologia\", \"Meio de Acesso\",\n",
        "    ]\n",
        "    for c in string_cols:\n",
        "        if c in df.columns:\n",
        "            df = df.withColumn(c, col(c).cast(StringType()))\n",
        "    if \"Acessos\" in df.columns:\n",
        "        df = df.withColumn(\"Acessos\", col(\"Acessos\").cast(DoubleType()))\n",
        "    if \"UF\" in df.columns:\n",
        "        df = df.withColumn(\"Região\", uf_to_region_udf(col(\"UF\")))\n",
        "    return df\n",
        "\n",
        "\n",
        "def process_colunas_csv(df):\n",
        "    value_cols = [\n",
        "        c for c in df.columns if c not in [\n",
        "            \"CNPJ\", \"Município\", \"UF\", \"Faixa de Velocidade\",\n",
        "            \"Tecnologia\", \"Empresa\", \"Porte da Prestadora\",\n",
        "            \"Código IBGE Município\", \"Grupo Econômico\", \"Meio de Acesso\",\n",
        "        ]\n",
        "    ]\n",
        "    exploded = df.select(\n",
        "        \"*\",\n",
        "        explode(\n",
        "            array(\n",
        "                *[\n",
        "                    struct(lit(c).alias(\"AnoMes\"), col(c).alias(\"Acessos\"))\n",
        "                    for c in value_cols\n",
        "                ]\n",
        "            )\n",
        "        ).alias(\"tmp\")\n",
        "    ).select(\n",
        "        col(\"CNPJ\"), col(\"Município\"), col(\"UF\"), col(\"Faixa de Velocidade\"),\n",
        "        col(\"Tecnologia\"), col(\"Empresa\"), col(\"Porte da Prestadora\"),\n",
        "        col(\"Código IBGE Município\"), col(\"Grupo Econômico\"), col(\"Meio de Acesso\"),\n",
        "        col(\"tmp.AnoMes\"), col(\"tmp.Acessos\"),\n",
        "    )\n",
        "\n",
        "    exploded = exploded.withColumn(\"Ano\", split(col(\"AnoMes\"), \"-\").getItem(0).cast(StringType()))\n",
        "    exploded = exploded.withColumn(\"Mês\", month_udf(split(col(\"AnoMes\"), \"-\").getItem(1)))\n",
        "    exploded = exploded.drop(\"AnoMes\")\n",
        "    exploded = exploded.withColumn(\"Acessos\", col(\"Acessos\").cast(DoubleType()))\n",
        "    exploded = exploded.withColumn(\"Região\", uf_to_region_udf(col(\"UF\")))\n",
        "    return exploded\n",
        "\n",
        "\n",
        "def process_densidade_csv(df):\n",
        "    string_cols = [\n",
        "        \"Ano\", \"Mês\", \"UF\", \"Município\", \"Código IBGE\", \"Nível Geográfico Densidade\",\n",
        "    ]\n",
        "    for c in string_cols:\n",
        "        if c in df.columns:\n",
        "            df = df.withColumn(c, col(c).cast(StringType()))\n",
        "    if \"Mês\" in df.columns:\n",
        "        df = df.withColumn(\"Mês\", month_udf(col(\"Mês\")))\n",
        "    if \"UF\" in df.columns:\n",
        "        df = df.withColumn(\"Região\", uf_to_region_udf(col(\"UF\")))\n",
        "    if \"Densidade\" in df.columns:\n",
        "        df = df.withColumn(\"Densidade\", col(\"Densidade\").cast(DoubleType()))\n",
        "    return df\n",
        "\n",
        "\n",
        "def process_total_csv(df):\n",
        "    if \"Ano\" in df.columns:\n",
        "        df = df.withColumn(\"Ano\", col(\"Ano\").cast(StringType()))\n",
        "    if \"Mês\" in df.columns:\n",
        "        df = df.withColumn(\"Mês\", month_udf(col(\"Mês\")))\n",
        "    if \"Acessos\" in df.columns:\n",
        "        df = df.withColumn(\"Acessos\", col(\"Acessos\").cast(LongType()))\n",
        "    return df\n",
        "\n",
        "\n",
        "if \"Contents\" not in response:\n",
        "    print(\"No files found in S3\")\n",
        "else:\n",
        "    zip_files = [obj[\"Key\"] for obj in response[\"Contents\"] if obj[\"Key\"].endswith(\".zip\")]\n",
        "    print(\"Found ZIP files in S3:\", zip_files)\n",
        "\n",
        "    for zip_key in zip_files:\n",
        "        print(f\"\\nProcessing {zip_key}...\")\n",
        "        local_zip = f\"/tmp/{os.path.basename(zip_key)}\"\n",
        "        s3.download_file(BUCKET_NAME, zip_key, local_zip)\n",
        "\n",
        "        with zipfile.ZipFile(local_zip, \"r\") as z:\n",
        "            for file_name in z.namelist():\n",
        "                print(f\" - Extracting {file_name}\")\n",
        "                local_file = f\"/tmp/{os.path.basename(file_name)}\"\n",
        "                with z.open(file_name) as f:\n",
        "                    with open(local_file, \"wb\") as out_f:\n",
        "                        out_f.write(f.read())\n",
        "\n",
        "                if file_name.lower().endswith(\".csv\"):\n",
        "                    df = spark.read.csv(local_file, header=True, sep=\";\", inferSchema=True)\n",
        "\n",
        "                    if file_name.lower().endswith(\"_colunas.csv\"):\n",
        "                        df = process_colunas_csv(df)\n",
        "                    elif \"densidade\" in file_name.lower():\n",
        "                        df = process_densidade_csv(df)\n",
        "                    elif \"total\" in file_name.lower():\n",
        "                        df = process_total_csv(df)\n",
        "                    else:\n",
        "                        df = process_normal_csv(df)\n",
        "\n",
        "                    csv_local = f\"/tmp/{os.path.splitext(os.path.basename(file_name))[0]}\"\n",
        "                    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).option(\"encoding\", \"UTF-8\").csv(csv_local)\n",
        "\n",
        "                    csv_file = glob.glob(f\"{csv_local}/*.csv\")[0]\n",
        "                    with open(csv_file, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read()\n",
        "                    with open(csv_file, 'w', encoding='utf-8-sig') as f:\n",
        "                        f.write(content)\n",
        "\n",
        "                    s3_key = f\"processed_data/{os.path.splitext(os.path.basename(file_name))[0]}.csv\"\n",
        "                    s3.upload_file(csv_file, BUCKET_NAME, s3_key)\n",
        "                    print(f\"Processed file saved in s3://{BUCKET_NAME}/{s3_key}\")\n",
        "                else:\n",
        "                    s3_key = f\"processed_data/{os.path.basename(file_name)}\"\n",
        "                    s3.upload_file(local_file, BUCKET_NAME, s3_key)\n",
        "                    print(f\"File uploaded without processing: s3://{BUCKET_NAME}/{s3_key}\")"
      ],
      "metadata": {
        "id": "VilVsnh0Bzz0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": null
        }
      },
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "3"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Browser_extraction",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}