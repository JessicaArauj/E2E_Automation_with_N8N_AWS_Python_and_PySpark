{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "72e825bc-119c-4a37-9a86-cf6a371655d4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_thRKIGYKewu"
      },
      "source": [
        "# Importing required libraries\n",
        "Organized according to PEP8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "465e1731-fa78-4ff1-951d-212957f777e9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jrzOsiLkKewy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02211e73-3b91-4842-b172-2d60e6e00cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.0 in /usr/local/lib/python3.12/dist-packages (3.5.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (1.40.27)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.27 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.40.27)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from boto3) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.27->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.27->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.27->boto3) (1.17.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (1.40.27)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.27 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.40.27)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from boto3) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.27->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.27->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.27->boto3) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#!apt-get install openjdk-11-jdk -y\n",
        "!pip install pyspark==3.5.0 boto3\n",
        "!pip install boto3\n",
        "\n",
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "import boto3\n",
        "import glob\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array, struct, explode, col, lit, split, udf\n",
        "from pyspark.sql.types import IntegerType, DoubleType, StringType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AWS_ACCESS_KEY_ID = \"AKIAS2CVJL3DS34J2OMJ\"\n",
        "AWS_SECRET_ACCESS_KEY = \"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\"\n",
        "AWS_REGION = \"sa-east-1\"\n",
        "BUCKET_NAME = \"fiec-teste-buckets\"\n",
        "PREFIX = \"file/\"\n",
        "\n",
        "s3 = boto3.client(\n",
        "    \"s3\",\n",
        "    region_name=AWS_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        ")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"S3DataTransformation\").getOrCreate()\n",
        "\n",
        "months_map = {\n",
        "    \"1\": \"Janeiro\", \"2\": \"Fevereiro\", \"3\": \"Março\", \"4\": \"Abril\",\n",
        "    \"5\": \"Maio\", \"6\": \"Junho\", \"7\": \"Julho\", \"8\": \"Agosto\",\n",
        "    \"9\": \"Setembro\", \"10\": \"Outubro\", \"11\": \"Novembro\", \"12\": \"Dezembro\"\n",
        "}\n",
        "month_udf = udf(lambda m: months_map.get(str(int(m)), m) if m else None, StringType())\n",
        "\n",
        "uf_to_region = {\n",
        "    \"AC\": \"Norte\",\"AP\": \"Norte\",\"AM\": \"Norte\",\"PA\": \"Norte\",\"RO\": \"Norte\",\"RR\": \"Norte\",\"TO\": \"Norte\",\n",
        "    \"MA\": \"Nordeste\",\"PI\": \"Nordeste\",\"CE\": \"Nordeste\",\"RN\": \"Nordeste\",\"PB\": \"Nordeste\",\n",
        "    \"PE\": \"Nordeste\",\"AL\": \"Nordeste\",\"SE\": \"Nordeste\",\"BA\": \"Nordeste\",\n",
        "    \"MG\": \"Sudeste\",\"SP\": \"Sudeste\",\"RJ\": \"Sudeste\",\"ES\": \"Sudeste\",\n",
        "    \"PR\": \"Sul\",\"RS\": \"Sul\",\"SC\": \"Sul\",\n",
        "    \"MT\": \"Centro-Oeste\",\"MS\": \"Centro-Oeste\",\"GO\": \"Centro-Oeste\",\"DF\": \"Centro-Oeste\"\n",
        "}\n",
        "\n",
        "def uf_to_region_func(uf):\n",
        "    if uf == \"Brasil\":\n",
        "        return \"Brasil\"\n",
        "    return uf_to_region.get(uf, \"Unknown\")\n",
        "\n",
        "uf_to_region_udf = udf(uf_to_region_func, StringType())\n",
        "\n",
        "def process_normal_csv(df):\n",
        "    if \"Ano\" in df.columns:\n",
        "        df = df.withColumn(\"Ano\", col(\"Ano\").cast(StringType()))\n",
        "    if \"Mês\" in df.columns:\n",
        "        df = df.withColumn(\"Mês\", month_udf(col(\"Mês\")))\n",
        "    string_cols = [\n",
        "        \"Grupo Econômico\",\"Empresa\",\"CNPJ\",\"Porte da Prestadora\",\n",
        "        \"UF\",\"Município\",\"Código IBGE Município\",\"Faixa de Velocidade\",\n",
        "        \"Tecnologia\",\"Meio de Acesso\"\n",
        "    ]\n",
        "    for c in string_cols:\n",
        "        if c in df.columns:\n",
        "            df = df.withColumn(c, col(c).cast(StringType()))\n",
        "    if \"Acessos\" in df.columns:\n",
        "        df = df.withColumn(\"Acessos\", col(\"Acessos\").cast(DoubleType()))\n",
        "    if \"UF\" in df.columns:\n",
        "        df = df.withColumn(\"Regiao\", uf_to_region_udf(col(\"UF\")))\n",
        "    return df\n",
        "\n",
        "def process_colunas_csv(df):\n",
        "    value_cols = [c for c in df.columns if c not in [\n",
        "        \"CNPJ\",\"Município\",\"UF\",\"Faixa de Velocidade\",\n",
        "        \"Tecnologia\",\"Empresa\",\"Porte da Prestadora\",\n",
        "        \"Código IBGE Município\",\"Grupo Econômico\",\"Meio de Acesso\"\n",
        "    ]]\n",
        "    exploded = df.select(\n",
        "        \"*\",\n",
        "        explode(\n",
        "            array(*[struct(lit(c).alias(\"AnoMes\"), col(c).alias(\"Acessos\")) for c in value_cols])\n",
        "        ).alias(\"tmp\")\n",
        "    ).select(\n",
        "        col(\"CNPJ\"), col(\"Município\"), col(\"UF\"), col(\"Faixa de Velocidade\"),\n",
        "        col(\"Tecnologia\"), col(\"Empresa\"), col(\"Porte da Prestadora\"),\n",
        "        col(\"Código IBGE Município\"), col(\"Grupo Econômico\"), col(\"Meio de Acesso\"),\n",
        "        col(\"tmp.AnoMes\"), col(\"tmp.Acessos\")\n",
        "    )\n",
        "    exploded = exploded.withColumn(\"Ano\", split(col(\"AnoMes\"), \"-\").getItem(0).cast(StringType()))\n",
        "    exploded = exploded.withColumn(\"Mês\", month_udf(split(col(\"AnoMes\"), \"-\").getItem(1)))\n",
        "    exploded = exploded.drop(\"AnoMes\")\n",
        "    exploded = exploded.withColumn(\"Acessos\", col(\"Acessos\").cast(DoubleType()))\n",
        "    exploded = exploded.withColumn(\"Regiao\", uf_to_region_udf(col(\"UF\")))\n",
        "    return exploded\n",
        "\n",
        "def process_densidade_csv(df):\n",
        "    if \"UF\" in df.columns:\n",
        "        df = df.withColumn(\"Regiao\", uf_to_region_udf(col(\"UF\")))\n",
        "    return df\n",
        "\n",
        "response = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=PREFIX)\n",
        "if \"Contents\" not in response:\n",
        "    print(\"No files found in S3\")\n",
        "else:\n",
        "    zip_files = [obj[\"Key\"] for obj in response[\"Contents\"] if obj[\"Key\"].endswith(\".zip\")]\n",
        "    print(\"Found ZIP files in S3:\", zip_files)\n",
        "\n",
        "    for zip_key in zip_files:\n",
        "        print(f\"\\nProcessing {zip_key}...\")\n",
        "        local_zip = f\"/tmp/{os.path.basename(zip_key)}\"\n",
        "        s3.download_file(BUCKET_NAME, zip_key, local_zip)\n",
        "\n",
        "        with zipfile.ZipFile(local_zip, \"r\") as z:\n",
        "            for file_name in z.namelist():\n",
        "                print(f\" - Extracting {file_name}\")\n",
        "                local_file = f\"/tmp/{os.path.basename(file_name)}\"\n",
        "                with z.open(file_name) as f:\n",
        "                    with open(local_file, \"wb\") as out_f:\n",
        "                        out_f.write(f.read())\n",
        "\n",
        "                if file_name.lower().endswith(\".csv\"):\n",
        "                    df = spark.read.csv(local_file, header=True, sep=\";\", inferSchema=True)\n",
        "                    if file_name.lower().endswith(\"_colunas.csv\"):\n",
        "                        df = process_colunas_csv(df)\n",
        "                    elif \"densidade\" in file_name.lower():\n",
        "                        df = process_densidade_csv(df)\n",
        "                    else:\n",
        "                        df = process_normal_csv(df)\n",
        "\n",
        "                    csv_local = f\"/tmp/{os.path.splitext(os.path.basename(file_name))[0]}\"\n",
        "                    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).option(\"encoding\", \"UTF-8\").csv(csv_local)\n",
        "\n",
        "                    import glob\n",
        "                    csv_file = glob.glob(f\"{csv_local}/*.csv\")[0]\n",
        "                    with open(csv_file, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read()\n",
        "                    with open(csv_file, 'w', encoding='utf-8-sig') as f:\n",
        "                        f.write(content)\n",
        "\n",
        "                    s3_key = f\"{PREFIX}processed_data/{os.path.splitext(os.path.basename(file_name))[0]}.csv\"\n",
        "                    s3.upload_file(csv_file, BUCKET_NAME, s3_key)\n",
        "                    print(f\"Processed file saved in s3://{BUCKET_NAME}/{s3_key}\")\n",
        "                else:\n",
        "                    s3_key = f\"{PREFIX}processed_data/{os.path.basename(file_name)}\"\n",
        "                    s3.upload_file(local_file, BUCKET_NAME, s3_key)\n",
        "                    print(f\"File uploaded without processing: s3://{BUCKET_NAME}/{s3_key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VilVsnh0Bzz0",
        "outputId": "36668b6e-7f49-46d7-b4b4-d1a71ea7f946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ZIP files in S3: ['file/acessos_banda_larga_fixa.zip']\n",
            "\n",
            "Processing file/acessos_banda_larga_fixa.zip...\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2007-2010.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2007-2010.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2007-2010_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2007-2010_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2011-2012.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2011-2012.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2011-2012_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2011-2012_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2013-2014.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2013-2014.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2013-2014_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2013-2014_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2015-2016.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2015-2016.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2015-2016_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2015-2016_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2017-2018.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2017-2018.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2017-2018_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2017-2018_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2019-2020.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2019-2020.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2019_2020_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2019_2020_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2021.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2021.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2021_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2021_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2022.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2022.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2022_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2022_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2023.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2023.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2023_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2023_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2024.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2024.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2024_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2024_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2025.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2025.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_2025_Colunas.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_2025_Colunas.csv\n",
            " - Extracting Acessos_Banda_Larga_Fixa_Total.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Acessos_Banda_Larga_Fixa_Total.csv\n",
            " - Extracting Densidades.pdf\n",
            "File uploaded without processing: s3://fiec-teste-buckets/file/processed_data/Densidades.pdf\n",
            " - Extracting Densidade_Banda_Larga_Fixa.csv\n",
            "Processed file saved in s3://fiec-teste-buckets/file/processed_data/Densidade_Banda_Larga_Fixa.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": null
        }
      },
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "3"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Browser_extraction",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}